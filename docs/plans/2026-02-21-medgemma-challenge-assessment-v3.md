# MedGemma Impact Challenge — Assessment v3 (Critical Review)

**Date**: 2026-02-21
**Supersedes**: FINAL.md assessment sections
**Purpose**: Identify flaws, gaps, and blind spots in the current FINAL.md plan using additional competition intelligence, codebase verification, and winning pattern analysis.

---

## Part 1: Flaws Found in Current FINAL.md

### FLAW 1 — CRITICAL: Video is the PRIMARY evaluation lens, yet deprioritized

**What the plan says**: Video recording is Day 3-4, Item #8 in P0 list (2h estimate).

**What the research reveals**: Multiple sources confirm:
> "Your video demo will be the primary lens through which judges evaluate your project, with your Writeup and application serving as verification and providing technical depth."

Furthermore, judging weight analysis from WebProNews shows:
- **Execution & Communication quality: 30%** (highest single criterion)
- HAI-DEF model use: 20%
- Real-world impact: 15%

The video + writeup quality is literally THE MOST IMPORTANT scoring factor. Yet the plan treats video as a late-stage recording task.

**Fix**:
1. **Write video script on Day 2** (not Day 3). Script every frame: what's shown, what's narrated, timing per segment.
2. **Record attempt 1 on Day 2 evening** (not Day 3). This gives Day 3 as buffer for re-recording.
3. Allocate **4h total** for video (not 2h): scripting (1h), first recording + review (1.5h), re-record + post-process (1.5h).
4. **Narration is mandatory**: A silent screen recording is not competitive. Plan voiceover (live narration via OBS, or record separately and overlay with ffmpeg).

### FLAW 2 — CRITICAL: Edge/local deployment narrative completely absent

**What the plan says**: Uses cloud-hosted HF Inference Endpoints exclusively.

**What the competition emphasizes**: The challenge framing explicitly calls out:
> "Healthcare environments where large closed models and constant internet connectivity are not practical — systems that can run locally, respect data privacy, and work without cloud infrastructure."

The current plan uses paid HF Inference Endpoints that require internet + subscription. This directly contradicts the competition's favored narrative. Winning projects in Gemma 3n (Vite Vere won special prize) emphasized **offline/edge capability**.

**Fix**: Add a "Deployment Path" section to the writeup (Page 3) that addresses this:
1. Note MedGemma is **open-weight** — can be downloaded and run locally with `transformers` or Ollama
2. Mention the 4B model fits on consumer GPUs (RTX 3090, 16GB)
3. Frame the HF Inference Endpoint as **development convenience**, not production architecture
4. Add 2 sentences to the writeup: "For production deployment, MedGemma runs locally on a single GPU, eliminating cloud dependency and ensuring patient data never leaves the hospital network."
5. This costs zero implementation time but dramatically improves the deployment narrative.

### FLAW 3 — SIGNIFICANT: Profile data labeling is dishonest

**What the plan says** (Line 249): "Label as 'Key Facts (MedGemma-extracted)' in UI. Honest — data was extracted by MedGemma."

**What the codebase reveals**: `nsclc_trial_profiles.json` was generated by **Gemini 2.5 Pro** (metadata in file: `"model": "gemini-2.5-pro"`, `"generated_at": "2026-02-20T22:27:03Z"`).

Labeling Gemini-generated data as "MedGemma-extracted" is factually incorrect. If judges inspect the JSON metadata, this undermines credibility on the "honesty" narrative that the plan explicitly relies on.

**Fix**: Label as either:
- "Key Facts (AI-extracted)" — generic but honest
- "Key Facts (structured from clinical note)" — focuses on the process, not the model
- If we want to show MedGemma doing extraction, build a minimal live INGEST step that actually calls MedGemma (but given time constraints, the generic label is safer)

### FLAW 4 — SIGNIFICANT: Plan is stale on already-completed items

**What the plan says**:
- Item #2 "Thin adapter (profile_adapter.py + unit tests) | 1h | Day 1 | **TODO**"

**What the codebase shows**: `profile_adapter.py` is FULLY IMPLEMENTED with 15 passing tests. This was completed but the plan wasn't updated, suggesting the plan document is drifting from reality.

**Fix**: Update FINAL.md to mark completed items. More importantly, this staleness means: any decision based on "hours remaining" calculations in the plan is wrong — we have ~1h more than the plan thinks.

### FLAW 5 — SIGNIFICANT: Test count claim is wrong

**What the plan says**: "140-test codebase" and "140 tests passing" (multiple locations).

**What the codebase shows**: **164 unit tests passing** (verified by running `uv run pytest tests/unit/`).

**Fix**: Update all references to 164. This is a free credibility boost.

### FLAW 6 — MINOR: "3 pages" is inline Writeup content, not a separate document

**What the plan implies**: The 3-page writeup is treated as a separate document to be created on Day 4.

**What Kaggle Writeups actually are**: The writeup IS the Kaggle Writeup post body. It's markdown, rendered inline on Kaggle's platform. Not a PDF, not a separate upload.

**Impact**:
- Must be markdown-native (not Word/PDF converted)
- Images must be hosted (uploaded to Kaggle or linked via URL)
- Architecture diagrams need to be rendered PNGs, not Mermaid code blocks
- Rich embedded cards (GitHub links, HF model pages) render automatically — use them

**Fix**: Plan the writeup as markdown from the start. Prepare PNG renders of architecture diagrams.

---

## Part 2: Gaps Not Addressed in Any Version

### GAP 1 — CRITICAL: No video narration plan

The 3-minute demo video NEEDS voiceover. Judges cannot understand a silent screen recording of a Streamlit app. Yet nowhere in v1, v2, or FINAL.md is narration mentioned.

**What winning submissions do**: Every successful hackathon demo video has:
1. A brief problem statement (15-30 seconds)
2. Live application walkthrough with narration explaining each step
3. Brief results/impact summary at the end

**Fix**: Write a video script with timed segments:

| Time | Visual | Narration |
|------|--------|-----------|
| 0:00-0:30 | Title card + problem stats | "Clinical trial matching... <5% of cancer patients find matching trials..." |
| 0:30-1:00 | Select patient, show key facts | "We start with a patient's clinical notes. MedGemma extracts structured key facts..." |
| 1:00-2:00 | PRESCREEN agent running | "Our agent autonomously searches ClinicalTrials.gov. Watch the reasoning chain..." |
| 2:00-2:30 | VALIDATE results | "For each matching trial, we evaluate every eligibility criterion..." |
| 2:30-3:00 | Benchmark dashboard | "Against expert annotations, our system achieves..." |

**Recording approach**: Use OBS (not Playwright) for video + live narration simultaneously. Playwright for QA only.

### GAP 2 — CRITICAL: Video hosting step missing

**What the plan says**: Record with Playwright, post-process with ffmpeg → submit.

**What's actually required**: Video must be **publicly hosted** (YouTube, Loom, etc.) and viewable without login. Then the URL is attached to the Kaggle Writeup.

**Missing steps**:
1. Create/configure YouTube (unlisted) or Loom account
2. Upload video after post-processing
3. Verify it's viewable without login
4. Attach link to Kaggle Writeup

**Fix**: Add these steps to Day 3/4 timeline. Account for upload time (~15 min) and verification.

### GAP 3 — CRITICAL: No early submission strategy

**What the plan says**: Single submission on Day 4 evening.

**What the platform allows**: Writeups can be revised and resubmitted multiple times before deadline.

**Risk**: A single Day 4 submission is a single point of failure. Platform issues, upload problems, or formatting bugs could cause a missed deadline.

**Fix**: Submit a **draft Writeup on Day 2** with placeholder content. This:
1. Validates the submission mechanism works
2. Ensures you have SOMETHING submitted even if Day 3-4 goes wrong
3. Allows iterative refinement
4. Reduces Day 4 stress from "must submit" to "polish and update"

### GAP 4 — SIGNIFICANT: Patient data privacy framing completely absent

Health AI judges care deeply about data privacy. The plan mentions zero privacy considerations.

**Fix**: Add to writeup Page 3 (Impact section) and video narration:
- "Patient data never leaves the local environment"
- "MedGemma's open-weight architecture enables on-premise deployment"
- "No patient data is sent to cloud APIs in production"
- Note: For the demo, we use synthetic/anonymized clinical notes, which is fine

### GAP 5 — SIGNIFICANT: Competitive landscape analysis missing

**What we know**: 4,100+ entrants across 61 teams. The 1st place winner of Gemma 3n Impact Challenge (same format) was **POIG — Precision Oncology Interface Gemma** — an oncology-focused AI system. This is EXACTLY our domain.

**Risk**: We're competing in a domain that already won the predecessor challenge. Our submission needs to differentiate from what POIG likely did.

**Fix**: Research POIG's approach (it won at Google France Health AI Hackathon) and differentiate on:
- **Agent-based workflow** (POIG likely didn't have a live CT.gov agent)
- **Multi-model orchestration** (4B + 27B + Gemini is more complex)
- **Benchmark transparency** (quantitative comparison vs GPT-4 baseline)
- **Special award target** (Agent-Based Workflows, not main track only)

### GAP 6 — SIGNIFICANT: No ablation analysis

**Best practice from research**: "Ablation studies matter — show what each component contributes."

**What the plan has**: Only model-level comparison (4B vs 27B vs Gemini). Does not show:
- PRESCREEN with vs without MedGemma normalization
- VALIDATE with vs without criterion-type-aware prompting
- Pipeline with vs without CWA (Closed World Assumption)

**Fix**: The CWA fix already has before/after data (60% → 75% for Gemini). Add a mini-ablation table:

| Configuration | Accuracy | Notes |
|---------------|----------|-------|
| Gemini + old labels | 60% | Before prompt fix |
| Gemini + native labels + CWA | 75% | After prompt fix |
| MedGemma 1.5 4B + old labels | 55% | Before prompt fix |
| MedGemma 1.5 4B + native labels + CWA | TBD | Post-fix rerun |

This shows the SYSTEM contributes, not just the model choice.

### GAP 7 — SIGNIFICANT: HF endpoint costs not budgeted

**What the codebase reveals**: MedGemma 27B is deployed on A100 80GB (self-deployed HF Inference Endpoint).

**Cost**: ~$5/hr. Over 4 days continuously running = **$480**.

**Fix**:
1. Shut down 27B endpoint when not actively benchmarking/demoing
2. Pre-compute and cache all demo results before shutting down
3. Budget: benchmark run (2h = $10) + demo recording (1h = $5) + buffer (2h = $10) = ~$25

### GAP 8 — MINOR: Regulatory awareness missing

**Best practice**: Winning health AI submissions briefly mention FDA/CE pathway awareness.

**Fix**: Add 1 sentence to Limitations section: "Clinical deployment would require FDA 510(k) clearance as a clinical decision support tool, and validation on diverse patient populations beyond NSCLC."

### GAP 9 — MINOR: No mention of dual-track submission strategy

**What the rules allow**: One entry to main track + one special award category submission, without duplicating materials.

**Fix**: Explicitly plan to submit to BOTH:
1. Main track (overall $75K pool)
2. Agent-Based Workflows special award

This doubles chances with zero additional work.

---

## Part 3: Reasoning Flaws in Prior Assessments

### Reasoning Flaw 1: "Honest benchmarking" is necessary but not sufficient

All three plan versions lean heavily on "judges value intellectual honesty" as the narrative anchor. This is true but overused as a crutch. Honesty about 55% accuracy doesn't win competitions — it just prevents disqualification.

**What actually wins** (from Gemma 3n analysis):
1. **Solving a specific, narrow problem** for an underserved population
2. **Visual, demo-able applications** with clear UX
3. **Working end-to-end** from input to actionable output

The plan's narrative should be: "We built a **working system** that orchestrates multiple AI models to do something no single model can do alone — automatically match cancer patients to clinical trials using live government databases." The honesty about benchmark numbers is a supporting detail, not the headline.

### Reasoning Flaw 2: Over-indexing on model accuracy metrics

The plan dedicates significant space to accuracy/F1/kappa numbers. But:
- HAI-DEF model use is only **20%** of the judging weight
- Judges evaluate "how models are applied in realistic contexts," not isolated metrics
- Phase 0 is only 20 pairs — not statistically significant enough to make strong claims

**Fix**: Frame benchmark as "directional validation" not "proof of superiority." Spend the saved writeup space on:
- Agent workflow visualization (for special award)
- Deployment path narrative (for real-world impact criterion)
- UX quality screenshots (for execution criterion — 30% weight)

### Reasoning Flaw 3: Assuming Streamlit is universally "faster" to build

The plan states "Streamlit: 10x faster than Next.js." This is true for prototyping, but Streamlit has known limitations for demo recordings:
- **No fine-grained control over animations/transitions** — everything is re-render
- **Async is painful** — Streamlit reruns the entire script on each interaction
- **st.status() is the only streaming mechanism** — limited visual appeal
- **Hard to make look polished** — judges comparing against Google's own HAI-DEF concept apps (built with modern web tech)

This isn't a "change to Next.js" recommendation — it's too late for that. But the plan should:
1. Invest in CSS customization (`st.markdown` with `unsafe_allow_html=True`)
2. Use `st.columns()` for layout polish
3. Test that the recording looks good at 1280x720

### Reasoning Flaw 4: Treating CWA/label fix as settled

The plan says "Prompt fix added criterion-type-aware instructions... Results pending." But the 4B and 27B re-runs are still running. If the prompt fix **doesn't** improve 4B/27B scores significantly, the entire narrative strategy needs revision again.

**Contingency not addressed**: What if post-fix 4B is still ~55-60% and 27B is ~60-65%? The plan says "reframe as honest analysis" but doesn't detail HOW this changes the writeup structure.

**Fix**: Define two narrative branches:
- **Branch A (27B ≥ 70%)**: "MedGemma 27B achieves competitive performance, validating domain-specific pretraining. The 4B model excels at specialized tasks (normalization) while 27B handles complex reasoning."
- **Branch B (27B < 65%)**: "Our systematic evaluation reveals that medical domain pretraining alone doesn't solve criterion evaluation — effective prompt engineering (CWA, type-aware instructions) provides larger gains than model scaling. This insight is actionable for the community."

---

## Part 4: Revised Priority Matrix (Execution & Communication First)

Given that **Execution & Communication quality is 30%** of judging, re-prioritize:

### P0 — Submission-blocking (reordered by judging impact)

| # | Item | Hours | Day | Judging Impact |
|---|------|-------|-----|----------------|
| 1 | **Wire Streamlit demo end-to-end** (PRESCREEN + VALIDATE in UI) | 4h | 1 | Execution (30%) |
| 2 | **Cached/replay mode** (record one live run for reliable demo) | 1.5h | 1 | Execution (30%) |
| 3 | **Write video script** (timed narration for 3-min demo) | 1h | 2 | Execution (30%) |
| 4 | **Record demo video** (OBS + voiceover, upload to YouTube) | 2h | 2 | Execution (30%) |
| 5 | **Draft Kaggle Writeup shell** + submit placeholder | 1h | 2 | Execution (30%) |
| 6 | **Benchmark dashboard page** (metrics table + confusion matrices) | 2h | 2 | HAI-DEF use (20%) |
| 7 | **Finalize 3-page writeup content** (markdown, diagrams as PNG) | 3h | 3 | All criteria |
| 8 | **Re-record video if needed** + post-process with ffmpeg | 1.5h | 3 | Execution (30%) |
| 9 | **Final Kaggle Writeup revision** + submit | 1h | 4 | Execution (30%) |

### P1 — Strong differentiator

| # | Item | Hours | Day | Judging Impact |
|---|------|-------|-----|----------------|
| 10 | Agent trace visualization (tool calls, reasoning chain) | 2h | 1-2 | Special Award |
| 11 | UI polish (CSS customization, loading states) | 2h | 3 | Execution (30%) |
| 12 | Error pattern analysis (mini-ablation: CWA before/after) | 1h | 3 | HAI-DEF use (20%) |
| 13 | Pre-warm script + endpoint health check | 0.5h | 3 | Technical (%) |

### P2 — If time permits

| # | Item | Hours | Day |
|---|------|-------|-----|
| 14 | Reproducibility: .env.example, README | 1h | 4 |
| 15 | MedGemma 1.5 4B multimodal | 3h | — |
| 16 | TxGemma integration | 2h | — |

---

## Part 5: Revised Milestone Plan

| Day | Date | AM | PM | Eve |
|-----|------|----|----|-----|
| **1** | Feb 21 | Wire PRESCREEN into Streamlit + agent trace. Benchmark 27B running (other session). | Wire VALIDATE into Streamlit. Build cached/replay mode. | Test full pipeline flow end-to-end. Record one cached run. |
| **2** | Feb 22 | Benchmark dashboard. Fix data label honesty issue. | Write video script. **Record demo video take 1** (OBS + voiceover). Upload to YouTube. | **Submit draft Kaggle Writeup** with placeholder text + video link. Start writeup Page 1. |
| **3** | Feb 23 | **Finalize writeup** (Pages 1-3, insert benchmark numbers, render diagram PNGs). | UI polish. Re-record video if take 1 was poor. Playwright QA pass. | Update Kaggle Writeup with final content. |
| **4** | Feb 24 | Final QA. Code cleanup. .env.example. | **Final Kaggle Writeup revision**. Verify video link works without login. | **Submit final version**. Shut down HF endpoints. |

### Key Changes from FINAL.md:
1. **Video moved to Day 2** (not Day 3-4)
2. **Draft Writeup submitted Day 2** (not Day 4)
3. **Day 4 is polish + revision** (not first attempt)
4. **Narration explicitly planned** (OBS, not Playwright for recording)
5. **HF endpoint shutdown** planned for cost control

---

## Part 6: Updated Risk Register

| Risk | Likelihood | Impact | Mitigation | **NEW** |
|------|-----------|--------|------------|---------|
| MedGemma 27B underperforms (< 65%) | Medium | High | Two narrative branches prepared (see Part 3) | Yes |
| **Video narration quality poor** | Medium | **Critical** | Practice script 2x before recording. Have text overlay plan as backup. | **NEW** |
| **YouTube upload/access fails** | Low | **Critical** | Have Loom as backup. Test unlisted link in incognito browser. | **NEW** |
| Endpoint cold-start during demo | High | High | Pre-warm + cached replay mode | — |
| **Draft Writeup rejected by platform** | Low | High | Submit placeholder on Day 2 to validate mechanism works | **NEW** |
| CT.gov API rate limit | Medium | Medium | Pre-cache trial data for demo patients | — |
| Streamlit looks unprofessional | Medium | Medium | CSS customization, test at 1280x720 recording resolution | **NEW** |
| **27B endpoint cost overrun** | Medium | Medium | Budget $25 max. Shut down when not actively using. | **NEW** |
| Playwright video quality | Medium | Low | Use OBS for final recording. Playwright for QA only. | Updated |

---

## Part 7: Writeup Structure Improvements

### Revised 3-Page Markdown Structure

**Page 1: The Problem & Our Solution (~800 words)**

1.1 **Clinical Trial Matching Crisis** (200 words)
- <5% of cancer patients enroll in trials
- 80% of trials fail to recruit on time
- Manual screening: 2+ hours per patient per trial
- Stakes: missed match = lost treatment option for a dying patient

1.2 **TrialMatch: Multi-Model Clinical AI** (300 words)
- Three-stage pipeline: INGEST → PRESCREEN → VALIDATE
- MedGemma provides domain-specific medical understanding
- Gemini orchestrates agentic reasoning and tool use
- Together they automate what oncologists do manually today
- [Architecture diagram PNG]

1.3 **HAI-DEF Model Utilization** (200 words)
- MedGemma 1.5 4B: medical term normalization (PRESCREEN search_variants)
- MedGemma 27B: criterion-level eligibility evaluation (VALIDATE)
- Both models are **open-weight** — deployable on-premise in hospitals
- No patient data leaves the local environment

**NEW additions**:
- Deployment path sentence (edge/local narrative)
- Privacy framing

**Page 2: Agent Architecture & Implementation (~800 words)**

2.1 **PRESCREEN Agent** (300 words + diagram)
- Gemini orchestrates multi-turn agentic loop with 3 tools
- `normalize_medical_terms`: MedGemma generates CT.gov-optimized search variants
- `search_trials`: Real-time CT.gov API v2 queries
- `get_trial_details`: Detailed protocol retrieval
- Agent adapts strategy based on results
- [Agent sequence diagram PNG]

2.2 **VALIDATE Evaluator** (200 words)
- Criterion-type-aware prompting (inclusion vs exclusion semantics)
- Closed World Assumption for clinical rigor
- Structured JSON output: label, reasoning, evidence_sentences

2.3 **Ablation: Prompt Engineering Matters** (150 words + table)
- CWA + native labels: Gemini 60% → 75% accuracy
- This shows system design contributes as much as model choice

2.4 **Reproducibility** (150 words)
- 164 unit tests, deterministic sampling, version-pinned deps
- Every run persists to `runs/<run_id>/` with full artifacts
- Cached replay mode: works without API keys

**NEW additions**:
- Ablation table
- Updated test count (164, not 140)

**Page 3: Results, Impact & Future (~800 words)**

3.1 **Benchmark Results** (300 words + table + confusion matrix)
- Phase 0: 20-pair criterion-level evaluation
- Model comparison table (insert final numbers)
- Key finding: domain pretraining + prompt engineering = complementary gains

3.2 **Live Demo** (150 words)
- NSCLC patient → PRESCREEN finds trials → VALIDATE evaluates criteria → ranked results
- Real-world scenario: oncologist finds trials for a newly diagnosed patient in minutes, not hours

3.3 **Impact & Deployment Path** (200 words)
- Screening time: hours → minutes
- MedGemma is open-weight: runs on a single GPU in any hospital
- No cloud dependency, no patient data exposure
- Integration path: FHIR-compatible input, EHR system hooks
- "Clinical deployment would require FDA 510(k) clearance as a clinical decision support tool"

3.4 **Limitations & Future Work** (150 words)
- Current: 20-pair directional assessment (1,024-pair Tier A next)
- Text-only pipeline (4B multimodal planned)
- No IRB clinical validation yet
- Next: fine-tune MedGemma on TrialGPT data for domain-specific gain
- Expand to non-NSCLC cancer types

**NEW additions**:
- Deployment/privacy narrative (addresses edge AI competition framing)
- FDA mention (regulatory maturity signal)
- Honest limitation scope

---

## Part 8: Competition Differentiation Strategy

### vs POIG (1st place Gemma 3n, also oncology)

| Dimension | POIG (likely) | TrialMatch (ours) | Advantage |
|-----------|---------------|-------------------|-----------|
| Domain | Oncology decision support | Oncology trial matching | Tie |
| Agent architecture | Likely simple/none | Genuine multi-turn agent | **Us** |
| Live data | Probably static | Real-time CT.gov API | **Us** |
| Multi-model | Probably single model | 4B + 27B + Gemini | **Us** |
| Benchmark | Unknown | Quantitative vs GPT-4 | **Us** |
| Edge AI | Unknown (won at Google France) | Cloud-first (needs framing) | **Them** |

### Differentiation message:
"TrialMatch is the only submission that combines **live ClinicalTrials.gov search** via a genuine multi-turn AI agent with **quantitative benchmark validation** against expert annotations and GPT-4 baseline."

---

## Part 9: Summary of All Changes from FINAL.md

| # | Change | Type | Impact |
|---|--------|------|--------|
| 1 | Video moved to Day 2 (from Day 3-4) | Schedule | Critical |
| 2 | Video narration plan added | New gap | Critical |
| 3 | Video hosting (YouTube) step added | New gap | Critical |
| 4 | Draft Writeup submitted Day 2 (from Day 4) | Schedule | Critical |
| 5 | Edge/local deployment narrative added to writeup | New gap | Critical |
| 6 | Data label honesty fixed ("AI-extracted" not "MedGemma-extracted") | Flaw fix | Significant |
| 7 | Test count updated 140 → 164 | Flaw fix | Minor |
| 8 | Profile adapter marked as DONE (was TODO) | Stale plan | Minor |
| 9 | Ablation table added to writeup | New gap | Significant |
| 10 | Patient privacy framing added | New gap | Significant |
| 11 | FDA/regulatory sentence added | New gap | Minor |
| 12 | Dual-track submission (main + agent award) | New gap | Significant |
| 13 | HF endpoint cost budget added | New gap | Significant |
| 14 | Two narrative branches for benchmark results | Reasoning fix | Significant |
| 15 | OBS for recording (Playwright for QA only) | Approach change | Significant |
| 16 | Writeup as markdown-native (not PDF) | Format fix | Significant |
| 17 | Priority reordered by judging weight (30% execution) | Strategy | Critical |
| 18 | Competitive positioning vs POIG added | New gap | Significant |
| 19 | Architecture diagrams must be PNG renders | Format fix | Minor |

---

## Sources (Additional to FINAL.md)

- [MedGemma Impact Challenge Writeups](https://www.kaggle.com/competitions/med-gemma-impact-challenge/writeups) — submission mechanism
- [WebProNews: MedGemma 1.5 $100K Challenge](https://www.webpronews.com/medgemma-1-5-googles-open-ai-unlocks-3d-scans-and-clinical-speech-for-healthcare-builders/) — partial judging weights
- [Google Blog: Gemma 3n Impact Challenge Winners](https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/) — POIG won 1st for oncology
- [HAI-DEF Concept Apps](https://huggingface.co/collections/google/hai-def-concept-apps) — Google's "gold standard" demos
- [Vibe Code Hackathon Submission Guide](https://gist.github.com/bigsnarfdude/19be893b58c844cb43b2d30d07f31af0) — Kaggle Writeup mechanics
- [MedGemma 27B on HuggingFace](https://huggingface.co/google/medgemma-27b-text-it) — model availability confirmation
- [MedGemma Developer Docs](https://developers.google.com/health-ai-developer-foundations/medgemma) — model card, adaptation methods
- [Kaggle Solution Write-Up Documentation](https://www.kaggle.com/solution-write-up-documentation) — writeup format specs
- [Kaggle LinkedIn: MedGemma Challenge Announcement](https://www.linkedin.com/posts/kaggle_the-medgemma-impact-challenge-activity-7416949289333006336-GozM) — 4,100+ entrants

# MedPix Multimodal Benchmark — MedGemma 4B (Vertex AI) vs Gemini Flash
# Uses Vertex AI Model Garden for MedGemma 4B instead of HF Inference
# No TGI CUDA bug — can use higher max_tokens and better multimodal support

benchmark: medpix_multimodal
description: "MedGemma 4B (Vertex AI vLLM) vs Gemini 3 Flash — v2: image-first + simple prompt + temp=0"

prompt:
  medgemma_simple: true  # Use simplified prompt for MedGemma 4B (better instruction-following)

data:
  benchmark_file: data/benchmark/medpix_thorax_10.json
  phase: 1  # Phase 1 = 10 pilot cases

models:
  medgemma_4b:
    provider: vertex
    project_id: "gen-lang-client-0517724223"
    region: "us-central1"
    endpoint_id: "923518299076034560"
    model_name: "medgemma-4b-vertex"
    gpu_hourly_rate: 1.15  # 1x L4
    max_tokens: 2048  # Vertex vLLM — no TGI CUDA bug, can go higher!
    max_concurrent: 1

  gemini_pro:
    provider: google
    model_id: "gemini-3-flash-preview"
    max_tokens: 2048
    max_concurrent: 1

judge:
  model_id: "gemini-3-pro-preview"
  description: "LLM-as-judge for diagnosis semantic equivalence (Pro to avoid Flash self-judge bias)"

evaluation:
  metrics:
    - diagnosis_exact_match
    - diagnosis_substring_match
    - diagnosis_llm_judge
    - findings_rouge_l_recall
    - findings_rouge_l_precision
    - findings_rouge_l_f1

budget:
  max_cost_usd: 5.0
  warn_at_usd: 2.0

output:
  run_dir: "runs/"
  save_raw_responses: true
  save_per_case_scores: true

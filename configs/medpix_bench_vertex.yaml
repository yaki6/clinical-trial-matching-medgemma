# MedPix Multimodal Benchmark — Vertex MedGemma 4B vs Gemini 3 Flash
# Uses Vertex AI for MedGemma (no TGI CUDA bug, higher max_tokens)
# Uses paid API key for Gemini (no rate limits)
# Gemini 3.1 Pro as LLM-as-judge

benchmark: medpix_multimodal
description: "Compare MedGemma 4B (Vertex AI, multimodal) vs Gemini 3 Flash on radiology image analysis"

data:
  benchmark_file: data/benchmark/medpix_thorax_10.json
  phase: 1  # Phase 1 = 10 pilot cases

models:
  medgemma_4b:
    provider: vertex
    project_id: "gen-lang-client-0517724223"
    region: "us-central1"
    endpoint_id: "923518299076034560"
    model_name: "medgemma-4b-vertex"
    max_tokens: 1024  # Vertex vLLM — no TGI CUDA bug!
    max_concurrent: 1
    gpu_hourly_rate: 1.15  # 1x L4

  gemini_pro:
    provider: google
    model_id: "gemini-2.0-flash"
    max_tokens: 2048
    max_concurrent: 1

judge:
  model_id: "gemini-2.0-flash"
  description: "Gemini 2.0 Flash as LLM-as-judge for diagnosis semantic equivalence"

evaluation:
  metrics:
    - diagnosis_exact_match
    - diagnosis_substring_match
    - diagnosis_llm_judge
    - findings_rouge_l_recall
    - findings_rouge_l_precision
    - findings_rouge_l_f1

budget:
  max_cost_usd: 10.0
  warn_at_usd: 5.0

output:
  run_dir: "runs/"
  save_raw_responses: true
  save_per_case_scores: true
